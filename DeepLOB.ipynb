{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLOB: Deep Convolutional Neural Networks for Limit Order Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook is used to reconstruct a paper [2] published in IEEE Transactions on Singal Processing. I use FI-2010 [1] dataset and present how model architecture is constructed here. The dataset can be downloaded from: https://etsin.fairdata.fi/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649\n",
    "\n",
    "[1] Ntakaris A, Magris M, Kanniainen J, Gabbouj M, Iosifidis A. Benchmark dataset for mid‚Äêprice forecasting of limit order book data with machine learning methods. Journal of Forecasting. 2018 Dec;37(8):852-66. https://arxiv.org/abs/1705.03233\n",
    "\n",
    "[2] Zhang Z, Zohren S, Roberts S. DeepLOB: Deep convolutional neural networks for limit order books. IEEE Transactions on Signal Processing. 2019 Mar 25;67(11):3001-12. https://arxiv.org/abs/1808.03668"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately four million events for ten consecutive trading days for five stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.random import set_seed\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Flatten, Dense, Dropout, Activation, Input, Reshape, Conv2D, MaxPooling2D, LeakyReLU, concatenate\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(1)\n",
    "set_seed(2)\n",
    "\n",
    "# limit gpu usage for keras - WHY?\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the No Auction half of the dataset. \n",
    "\n",
    "The dataset offers multiple normalisation options, I use the z-score normalisation. \n",
    "\n",
    "The dataset is already split into training data and testing data. I use the first 6 days as training data, the 7th day as validation data and the last 3 days as testing data.\n",
    "\n",
    "The first 40 columns of the FI-2010 dataset are 10 levels of ask and bid information for a limit order book and I only use these 40 features in our network. The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons (up, down, stationary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(data):\n",
    "    df = data[:40, :].T\n",
    "    return df\n",
    "\n",
    "def get_labels(data):\n",
    "    df = data[-5:, :].T\n",
    "    return df\n",
    "\n",
    "def classification(data, labels, timestep): # train_trimmed, train_labels, 100\n",
    "    [N, D] = data.shape # Shape of trimmed features data [200,000, 40]\n",
    "    df = np.array(data) # Turn it into an array\n",
    "\n",
    "    dY = np.array(labels) # Turn labels data into an array\n",
    "\n",
    "    dataY = dY[timestep - 1:N] # Trim labels data [] \n",
    "\n",
    "    dataX = np.zeros((N - timestep + 1, timestep, D))  #[200,000 -99, 100, 40]\n",
    "    for i in range(timestep, N + 1): #[100, 200,000 + 1]\n",
    "        dataX[i - timestep] = df[i - timestep:i, :]\n",
    "\n",
    "    return dataX.reshape(dataX.shape + (1,)), dataY # explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'E:/New Downloads/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore'\n",
    "\n",
    "train_raw = np.loadtxt(data_path + '/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_6.txt')\n",
    "validation_raw = np.loadtxt(data_path + '/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_6.txt')\n",
    "test1_raw = np.loadtxt(data_path + '/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt')\n",
    "test2_raw = np.loadtxt(data_path + '/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt')\n",
    "test3_raw = np.loadtxt(data_path + '/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt')\n",
    "test_raw = np.hstack((test1_raw, test2_raw, test3_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant features from dataset\n",
    "train_trimmed = trim(train_raw)\n",
    "validation_trimmed = trim(validation_raw)\n",
    "test_trimmed = trim(test_raw)\n",
    "\n",
    "# Extract labels from dataset\n",
    "train_labels = get_labels(train_raw)\n",
    "validation_labels = get_labels(validation_raw)\n",
    "test_labels = get_labels(test_raw)\n",
    "\n",
    "# Prepare all data - create an array containing the values of all features for last 100 timesteps\n",
    "train_x, train_y = classification(train_trimmed, train_labels, 100)\n",
    "train_y = train_y[:,3] - 1 # Prediction horizon set to 3 events in the future, and labels are rescaled\n",
    "train_y = np_utils.to_categorical(train_y, 3)\n",
    "\n",
    "validation_x, validation_y = classification(validation_trimmed, validation_labels, 100)\n",
    "validation_y = validation_y[:,3] - 1 # Prediction horizon set to 3 events in the future, and labels are rescaled\n",
    "validation_y = np_utils.to_categorical(validation_y, 3)\n",
    "\n",
    "test_x, test_y = classification(test_trimmed, test_labels, 100)\n",
    "test_y = test_y[:,3] - 1 # Prediction horizon set to 3 events in the future, and labels are rescaled\n",
    "test_y = np_utils.to_categorical(test_y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of the architecture can be found in [2]. Briefly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(timestep, no_features, no_lstm):\n",
    "    input_layer = Input(shape=(timestep, no_features, 1))\n",
    "    \n",
    "    # Convolutional block - COMPLETE\n",
    "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_layer)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(32, (1, 10))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    \n",
    "    # Inception module - COMPLETE\n",
    "    convsecond_1 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    convsecond_1 = Conv2D(64, (3, 1), padding='same')(convsecond_1)\n",
    "    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "\n",
    "    convsecond_2 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    convsecond_2 = Conv2D(64, (5, 1), padding='same')(convsecond_2)\n",
    "    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "\n",
    "    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "    convsecond_3 = Conv2D(64, (1, 1), padding='same')(convsecond_3)\n",
    "    convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "    \n",
    "    convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "    \n",
    "    # use the MC dropout here - COMPLETE\n",
    "    \n",
    "    conv_reshape = Reshape((int(convsecond_output.shape[1]), int(convsecond_output.shape[3])))(convsecond_output)\n",
    "    \n",
    "    # LSTM layer\n",
    "    conv_lstm = CuDNNLSTM(no_lstm)(conv_reshape) # CuDNN is much faster than vanilla LSTM when run on GPU\n",
    "    \n",
    "    # Output layer\n",
    "    output_layer = Dense(3, activation='softmax')(conv_lstm)\n",
    "    \n",
    "    # Putting layers together\n",
    "    model = Model(inputs = input_layer, outputs = output_layer)\n",
    "    optimizer = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1) # Explain hyperparameters\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) # Explain\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 40, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 100, 20, 32)  96          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 100, 20, 32)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 100, 20, 32)  4128        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 100, 20, 32)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 100, 20, 32)  4128        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 100, 20, 32)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 100, 10, 32)  2080        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 100, 10, 32)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 100, 10, 32)  4128        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 100, 10, 32)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 100, 10, 32)  4128        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 100, 10, 32)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 100, 1, 32)   10272       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 100, 1, 32)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 100, 1, 32)   4128        leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 100, 1, 32)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 100, 1, 32)   4128        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 100, 1, 32)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 100, 1, 64)   2112        leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 100, 1, 64)   2112        leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 100, 1, 64)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 100, 1, 64)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 100, 1, 32)   0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 100, 1, 64)   12352       leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 100, 1, 64)   20544       leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 100, 1, 64)   2112        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 100, 1, 64)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 100, 1, 64)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 100, 1, 64)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100, 1, 192)  0           leaky_re_lu_10[0][0]             \n",
      "                                                                 leaky_re_lu_12[0][0]             \n",
      "                                                                 leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 100, 192)     0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)          (None, 64)           66048       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            195         cu_dnnlstm[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 142,691\n",
      "Trainable params: 142,691\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deeplob = create_model(100, 40, 64)\n",
    "deeplob.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain hyperparamters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplob.fit(train_x, train_y, epochs=5, batch_size=32, verbose=2, validation_data=(validation_x, validation_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "deeplob_json = deeplob.to_json()\n",
    "with open('deeplob_json', 'w') as json_file:\n",
    "    json_file.write(deeplob_json)\n",
    "\n",
    "# Serialize weights to HDF5\n",
    "deeplob.save_weights(\"deeplob.h5\")\n",
    "\n",
    "print(\"Model saved to disk once\")\n",
    "\n",
    "deeplob.save(\"C:/Users/hamza/Downloads/DeepLOB/model\")\n",
    "\n",
    "print(\"Model saved to disk twice\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLOB",
   "language": "python",
   "name": "deeplob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
